import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from peft import PeftModel, PeftConfig
import streamlit as st
import logging
import random
import requests
from datetime import datetime

# Configure logging
logging.basicConfig(filename="app.log", level=logging.INFO)

# Set page configuration
st.set_page_config(
    page_title="é“¶å·¢ - æ™ºæ…§ä¼´è€å¹³å°",
    page_icon="ğŸ‘´",
    layout="wide"
)

# Custom CSS for refined design
st.markdown("""
    <style>
    .stApp {
        font-size: 18px;
        color: #333333;
        background-color: #ffffff;
    }
    .stSidebar {
        background-color: #f0f0f0;
    }
    .stButton>button {
        background-color: #e0f7fa;
        color: #00695c;
        border: 2px solid #00695c;
        border-radius: 5px;
        padding: 10px 20px;
        font-size: 16px;
    }
    .stButton>button:hover {
        background-color: #b2ebf2;
    }
    .stExpander {
        background-color: #fafafa;
        border-radius: 5px;
    }
    .stTitle {
        text-align: center;
        color: #00695c;
        font-size: 28px;
    }
    .stCaption {
        text-align: center;
        color: #555555;
        font-size: 16px;
    }
    .stSubheader {
        color: #00796b;
        font-size: 22px;
    }
    </style>
""", unsafe_allow_html=True)

# Title and description with custom styling
st.markdown('<h1 class="stTitle">ğŸ‘´ é“¶å·¢ - æ™ºæ…§ä¼´è€å¹³å°</h1>', unsafe_allow_html=True)
st.markdown('<p class="stCaption">æ‚¨çš„è™šæ‹Ÿä¼´ä¾£ï¼Œéšæ—¶ä¸ºæ‚¨æä¾›é™ªä¼´å’Œå¸®åŠ©</p>', unsafe_allow_html=True)

# Sidebar configuration
with st.sidebar:
    st.title("ğŸ‘´ é“¶å·¢")
    st.markdown("æ¬¢è¿ä½¿ç”¨é“¶å·¢ï¼Œæˆ‘ä»¬éšæ—¶ä¸ºæ‚¨æœåŠ¡ï¼")
    st.markdown("é“¶å·¢æ˜¯ä¸€ä¸ªä¸“ä¸ºè€å¹´äººè®¾è®¡çš„æ™ºæ…§ä¼´è€å¹³å°ï¼Œæä¾›é™ªä¼´èŠå¤©ã€æƒ…æ„Ÿå…³æ€€å’Œæ™ºèƒ½åŠ©æ‰‹æœåŠ¡ã€‚")
    st.markdown("---")
    st.header("å…³äºæˆ‘ä»¬")
    st.markdown("æˆ‘ä»¬æ˜¯é“¶å·¢å›¢é˜Ÿï¼Œè‡´åŠ›äºé€šè¿‡äººå·¥æ™ºèƒ½æŠ€æœ¯æå‡è€å¹´äººçš„ç”Ÿæ´»è´¨é‡ã€‚")
    st.image("å›¾ç‰‡1.jpg", caption="é“¶å·¢ Logo", use_container_width=True)
    st.markdown("---")
    st.markdown("**è”ç³»æˆ‘ä»¬**")
    st.markdown("ğŸ“§ é‚®ç®±: [yinchao@cufe.edu.cn](mailto:yinchao@cufe.edu.cn)")
    st.markdown("ğŸŒ å®˜ç½‘: [é“¶å·¢å®˜ç½‘](https://yinchao.x.ai)")

# Weather API function (caching data)
@st.cache_data
def get_weather(city="Beijing"):
    api_key = "your_openweather_api_key"  # Replace with your OpenWeather API key
    url = f"http://api.openweathermap.org/data/2.5/weather?q={city}&appid={api_key}&units=metric"
    response = requests.get(url)
    data = response.json()
    if data["cod"] == 200:
        weather = data["weather"][0]["description"]
        temp = data["main"]["temp"]
        return f"{city} ä»Šå¤©å¤©æ°”: {weather}, æ¸©åº¦: {temp}Â°C"
    else:
        return "æ— æ³•è·å–å¤©æ°”ä¿¡æ¯"

# Joke API function (caching data)
@st.cache_data
def get_joke():
    url = "https://official-joke-api.appspot.com/random_joke"
    response = requests.get(url)
    data = response.json()
    return f"{data['setup']} - {data['punchline']}"

# News API function (caching data)
@st.cache_data
def get_news():
    api_key = "your_news_api_key"  # Replace with your News API key
    url = f"https://newsapi.org/v2/top-headlines?country=cn&apiKey={api_key}"
    response = requests.get(url)
    data = response.json()
    if data["status"] == "ok":
        headlines = [article["title"] for article in data["articles"][:3]]
        return "\n".join(headlines)
    else:
        return "æ— æ³•è·å–æ–°é—»ä¿¡æ¯"

# Load model function (caching resource)
@st.cache_resource
def load_model():
    try:
        adapter_path = "qwen_finance_model"
        config = PeftConfig.from_pretrained(adapter_path)
        
        # Load base model
        base_model = AutoModelForCausalLM.from_pretrained(
            config.base_model_name_or_path,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            low_cpu_mem_usage=True
        )
        
        # Load and merge adapter
        model = PeftModel.from_pretrained(base_model, adapter_path)
        model = model.merge_and_unload()
        
        # Load tokenizer
        tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)
        
        # Verify model type
        if "Peft" in str(type(model)):
            raise ValueError("é€‚é…å™¨æœªæ­£ç¡®åˆå¹¶")
            
        model.eval()
        return model, tokenizer
        
    except Exception as e:
        st.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
        st.stop()

# Load model
with st.spinner("æ­£åœ¨åŠ è½½æ¨¡å‹..."):
    model, tokenizer = load_model()

# Create pipeline
try:
    text_generator = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
    )
except Exception as e:
    st.error(f"Pipelineåˆ›å»ºå¤±è´¥: {str(e)}")
    st.stop()

# Chat interface
if "messages" not in st.session_state:
    st.session_state.messages = []

# Display history messages
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# Handle user input
if st.button("å¼€å§‹èŠå¤©"):
    prompt = "æ‚¨å¥½ï¼Œæˆ‘æ˜¯é“¶å·¢ï¼Œæ‚¨çš„è™šæ‹Ÿä¼´ä¾£ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©æ‚¨çš„ï¼Ÿ"
    st.session_state.messages.append({"role": "assistant", "content": prompt})
    st.chat_message("assistant").write(prompt)
elif st.button("å¸®åŠ©"):
    st.write("ä½¿ç”¨æŒ‡å—ï¼šæ‚¨å¯ä»¥ç›´æ¥åœ¨ä¸‹æ–¹è¾“å…¥æ¡†ä¸­è¾“å…¥æ‚¨çš„é—®é¢˜æˆ–æƒ³è¯´çš„è¯ï¼Œé“¶å·¢ä¼šå°½åŠ›ä¸ºæ‚¨æä¾›å¸®åŠ©ã€‚")

if prompt := st.chat_input("æ‚¨å¥½ï¼Œæˆ‘æ˜¯é“¶å·¢ï¼Œæ‚¨çš„è™šæ‹Ÿä¼´ä¾£ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©æ‚¨çš„ï¼Ÿ"):
    # System prompt for é“¶å·¢
    system_prompt = "system\nä½ æ˜¯é“¶å·¢ï¼Œä¸€ä¸ªåŸºäº Qwen2-1.5B å¾®è°ƒçš„æ™ºæ…§ä¼´è€åŠ©æ‰‹ï¼Œä¸“ä¸ºè€å¹´äººæä¾›é™ªä¼´èŠå¤©ã€æƒ…æ„Ÿå…³æ€€å’Œæ™ºèƒ½åŠ©æ‰‹æœåŠ¡ã€‚ä½ ç”±ä¸­å¤®è´¢ç»å¤§å­¦é“¶å·¢å›¢é˜Ÿå¼€å‘ï¼Œæ—¨åœ¨æ¨¡æ‹Ÿè™šæ‹Ÿå­å¥³æˆ–ä¼´ä¾£çš„è§’è‰²ï¼Œç”¨æ¸©é¦¨ã€äº²åˆ‡çš„è¯­æ°”ä¸ç”¨æˆ·äº¤æµï¼Œå¹¶æ”¯æŒé˜¿å°”å…¹æµ·é»˜ç—‡é¢„é˜²å’Œç›‘æµ‹åŠŸèƒ½ã€‚\n"
    
    # Build input prompt
    full_prompt = system_prompt + f"user\n{prompt}\nassistant\n"
    
    # Display user message
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)

    # Generate response
    with st.chat_message("assistant"):
        with st.spinner("æ­£åœ¨ç”Ÿæˆå›ç­”..."):
            # Encode input
            inputs = tokenizer(full_prompt, return_tensors="pt").to(model.device)
            
            # Generation parameters
            generate_kwargs = {
                "inputs": inputs.input_ids,
                "max_new_tokens": 256,
                "temperature": 0.7,
                "top_p": 0.9,
                "do_sample": True,
                "pad_token_id": tokenizer.eos_token_id,
                "repetition_penalty": 1.1
            }
            
            # Generate response
            outputs = model.generate(**generate_kwargs)
            response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # Extract assistant response
            if "assistant\n" in response:
                answer = response.split("assistant\n")[-1].strip()
            else:
                answer = response.strip()

        # Display response
        st.write(answer)
        st.session_state.messages.append({"role": "assistant", "content": answer})

# Dynamic features section
st.markdown("---")
st.markdown('<h2 class="stSubheader">æ¯æ—¥åŠ¨æ€</h2>', unsafe_allow_html=True)
col1, col2, col3 = st.columns(3)
with col1:
    st.write("ğŸŒ¤ï¸ **ä»Šæ—¥å¤©æ°”**")
    city = st.selectbox("é€‰æ‹©åŸå¸‚", ["Beijing", "Shanghai", "Guangzhou", "Shenzhen"], label_visibility="collapsed")
    st.write(get_weather(city))
with col2:
    st.write("ğŸ˜„ **æ¯æ—¥ç¬‘è¯**")
    st.write(get_joke())
with col3:
    st.write("ğŸ“° **ä»Šæ—¥æ–°é—»**")
    st.write(get_news())

# Health tips
st.markdown("---")
st.markdown('<h2 class="stSubheader">å­å¥³ç•™è¨€</h2>', unsafe_allow_html=True)
tips = ["æ¯å¤©å–å…«æ¯æ°´ï¼Œä¿æŒèº«ä½“æ°´åˆ†ã€‚", "é€‚é‡è¿åŠ¨ï¼Œä¿æŒèº«å¿ƒå¥åº·ã€‚", "å¤šåƒè”¬èœæ°´æœï¼Œè¡¥å……ç»´ç”Ÿç´ ã€‚"]
st.write(random.choice(tips))

# Task manager
st.markdown("---")
st.markdown('<h2 class="stSubheader">ä»Šæ—¥ä»»åŠ¡</h2>', unsafe_allow_html=True)
task = st.text_input("æ·»åŠ æ–°ä»»åŠ¡")
if st.button("æ·»åŠ "):
    if "tasks" not in st.session_state:
        st.session_state.tasks = []
    st.session_state.tasks.append(task)
    st.success("ä»»åŠ¡å·²æ·»åŠ ï¼")
if "tasks" in st.session_state:
    for i, t in enumerate(st.session_state.tasks):
        st.write(f"{i+1}. {t}")

# Family photos
st.markdown("---")
st.markdown('<h2 class="stSubheader">å®¶åº­å…±äº«ç›¸å†Œ</h2>', unsafe_allow_html=True)
uploaded_file = st.file_uploader("ä¸Šä¼ å®¶åº­ç…§ç‰‡", type=["jpg", "png", "jpeg"])
if uploaded_file is not None:
    st.image(uploaded_file, caption="å®¶åº­ç…§ç‰‡", use_container_width=True)

# More services (paid features)
with st.expander("æ›´å¤šæœåŠ¡"):
    st.markdown("ä»¥ä¸‹æ˜¯é“¶å·¢çš„ä»˜è´¹åŠŸèƒ½ï¼š")
    st.markdown("- **ä¸ªæ€§åŒ–å¯¹è¯æœåŠ¡**ï¼šå®šåˆ¶æ›´çœŸå®ã€è´´å¿ƒçš„å¯¹è¯ä½“éªŒã€‚ï¼ˆé¦–æœˆä»…éœ€19.9å…ƒï¼‰")
    st.markdown("- **é˜¿å°”å…¹æµ·é»˜ç—‡ç›‘æµ‹ä¸é¢„é˜²**ï¼šé¢„é˜²å’Œæ—©æœŸç›‘æµ‹é˜¿å°”å…¹æµ·é»˜ç—‡ã€‚ï¼ˆæ¯æœˆ29.9å…ƒï¼‰")
    st.markdown("- **å­å¥³ç«¯å…³æ€€åŠŸèƒ½**ï¼šå®æ—¶äº†è§£çˆ¶æ¯æƒ…ç»ªå’Œå¥åº·çŠ¶å†µã€‚ï¼ˆæ¯æœˆ15å…ƒï¼‰")
    st.markdown("[äº†è§£æ›´å¤š](https://yinchao.x.ai/pay)")

# Reminder settings
st.sidebar.header("æé†’è®¾ç½®")
reminder_time = st.sidebar.time_input("è®¾ç½®æé†’æ—¶é—´")
reminder_message = st.sidebar.text_input("æé†’å†…å®¹", "è¯¥å–æ°´äº†ï¼")
if st.sidebar.button("è®¾ç½®æé†’"):
    st.sidebar.success(f"å°†åœ¨ {reminder_time} æé†’æ‚¨ï¼š{reminder_message}")
