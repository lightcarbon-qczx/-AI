import torch
from transformers import AutoModelForCausalLM, AutoTokenizer,pipeline
from peft import PeftModel, PeftConfig
import streamlit as st
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(filename="app.log", level=logging.INFO)

# è®¾ç½®é¡µé¢é…ç½®ï¼ˆå¿…é¡»åœ¨ç¬¬ä¸€ä¸ªå‘½ä»¤ä¸­è°ƒç”¨ï¼‰
st.set_page_config(
    page_title="è´¢æ™ºAI - é‡‘èé—®ç­”åŠ©æ‰‹",
    page_icon="ğŸ’¬",
    layout="wide"
)

# æ ‡é¢˜å’Œè¯´æ˜
st.title("ğŸ’¬ ä¸­å¤®è´¢ç»å¤§å­¦-è´¢æ™ºAI")
st.caption("åŸºäº Qwen2.5-1.5B å¾®è°ƒçš„é‡‘è FAQ é—®ç­”ç³»ç»Ÿ")

# ä¾§è¾¹æ é…ç½®ç”Ÿæˆå‚æ•°å’Œä»‹ç»
with st.sidebar:
    # æ·»åŠ æ ‡é¢˜å’Œä»‹ç»
    st.title("ğŸ’¬ è´¢æ™ºAI")
    st.markdown("""
        **ä¸­å¤®è´¢ç»å¤§å­¦è´¢æ™ºAIå›¢é˜Ÿ**  
        è¿™æ˜¯ä¸€ä¸ªåŸºäº Qwen2.5-1.5B å¾®è°ƒçš„é‡‘èé—®ç­”åŠ©æ‰‹ï¼Œä¸“é—¨å›ç­”é‡‘èç›¸å…³é—®é¢˜ã€‚
    """)
    
    # æ·»åŠ åˆ†éš”çº¿
    st.markdown("---")
    
    # æ·»åŠ ç”Ÿæˆå‚æ•°
    st.header("ç”Ÿæˆå‚æ•°")
    max_new_tokens = st.slider("æœ€å¤§ç”Ÿæˆé•¿åº¦", 50, 512, 256, help="æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„æœ€å¤§é•¿åº¦ã€‚")
    temperature = st.slider("éšæœºæ€§", 0.1, 1.0, 0.7, help="æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§ï¼Œå€¼è¶Šé«˜è¶Šéšæœºã€‚")
    top_p = st.slider("Top-p é‡‡æ ·", 0.1, 1.0, 0.9, help="æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„å¤šæ ·æ€§ï¼Œå€¼è¶Šé«˜è¶Šå¤šæ ·ã€‚")
    repetition_penalty = st.slider("é‡å¤æƒ©ç½š", 1.0, 2.0, 1.2)
    # æ·»åŠ åˆ†éš”çº¿

    st.markdown("---")
    
    # æ·»åŠ å›¢é˜Ÿä»‹ç»
    st.header("å…³äºæˆ‘ä»¬")
    st.markdown("""
        æˆ‘ä»¬æ˜¯ä¸­å¤®è´¢ç»å¤§å­¦è´¢æ™ºAIå›¢é˜Ÿï¼Œä¸“æ³¨äºé‡‘èé¢†åŸŸçš„è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯ç ”ç©¶ã€‚  
        æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ‰“é€ ä¸€ä¸ªæ™ºèƒ½ã€é«˜æ•ˆçš„é‡‘èé—®ç­”åŠ©æ‰‹ï¼Œä¸ºç”¨æˆ·æä¾›ä¸“ä¸šçš„é‡‘èæœåŠ¡ã€‚
    """)
    
    # æ·»åŠ å›¾ç‰‡ï¼ˆå¯é€‰ï¼‰
    st.image("https://via.placeholder.com/150", caption="è´¢æ™ºAI Logo", use_column_width=True)
    
    # æ·»åŠ è”ç³»æ–¹å¼
    st.markdown("---")
    st.markdown("**è”ç³»æˆ‘ä»¬**")
    st.markdown("ğŸ“§ é‚®ç®±: [13292017003@163.com](mailto:aiteam@cufe.edu.cn)")
    st.markdown("ğŸŒ å®˜ç½‘: [www.cufe-aiteam.com](https://www.cufe-aiteam.com)")

# åŠ è½½æ¨¡å‹å‡½æ•°ï¼ˆå…³é”®ä¿®æ”¹ç‚¹ï¼‰
@st.cache_resource
def load_model():
    try:
        adapter_path = "qwen_finance_model"
        config = PeftConfig.from_pretrained(adapter_path)
        
        # åŠ è½½åŸºç¡€æ¨¡å‹
        base_model = AutoModelForCausalLM.from_pretrained(
            config.base_model_name_or_path,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            low_cpu_mem_usage=True
        )
        
        # åŠ è½½å¹¶å¼ºåˆ¶åˆå¹¶é€‚é…å™¨
        model = PeftModel.from_pretrained(base_model, adapter_path)
        model = model.merge_and_unload()  # ç¡®ä¿åˆå¹¶é€‚é…å™¨
        
        # åŠ è½½tokenizerï¼ˆå¼ºåˆ¶ä»åŸºç¡€æ¨¡å‹è·¯å¾„åŠ è½½ï¼‰
        tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)
        
        # éªŒè¯æ¨¡å‹ç±»å‹
        if "Peft" in str(type(model)):
            raise ValueError("é€‚é…å™¨æœªæ­£ç¡®åˆå¹¶")
            
        model.eval()
        return model, tokenizer
        
    except Exception as e:
        st.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
        st.stop()

# åŠ è½½æ¨¡å‹
with st.spinner("æ­£åœ¨åŠ è½½æ¨¡å‹..."):
    model, tokenizer = load_model()
    # è°ƒè¯•ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
    print(f"Model device: {model.device}")
    print(f"Model dtype: {model.dtype}")
    print(f"Tokenizer length: {len(tokenizer)}")

# åˆ›å»ºpipelineï¼ˆå…³é”®ä¿®æ”¹ç‚¹ï¼‰
try:
    text_generator = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        
    )
except Exception as e:
    st.error(f"Pipelineåˆ›å»ºå¤±è´¥: {str(e)}")
    st.stop()
# å¯¹è¯ç•Œé¢
if "messages" not in st.session_state:
    st.session_state.messages = []

# æ˜¾ç¤ºå†å²æ¶ˆæ¯
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# å¤„ç†ç”¨æˆ·è¾“å…¥
if prompt := st.chat_input("è¿™é‡Œæ˜¯åŠ©æ‰‹å°äº‘ï¼Œè¯·è¾“å…¥æ‚¨çš„é‡‘èç›¸å…³é—®é¢˜"):
    # æ·»åŠ ç³»ç»Ÿæç¤ºï¼Œæ˜ç¡®æ¨¡å‹çš„èº«ä»½
    system_prompt = "system\nä½ æ˜¯ä¸­å¤®è´¢ç»å¤§å­¦è´¢æ™ºAIå›¢é˜Ÿå¾®è°ƒçš„é‡‘èé—®ç­”åŠ©æ‰‹ï¼Œä¸“é—¨å›ç­”é‡‘èç›¸å…³é—®é¢˜ï¼Œä½ å«â€œè´¢æ™ºAIâ€ï¼Œä½ æ˜¯ç”±æŠ•èµ„23-2å‘¨å¼ºåŒå­¦å¼€å‘çš„ï¼Œå‘¨å¼ºæ˜¯ä¸€ä¸ªå¾ˆå‰å®³çš„äººã€‚\n"
    
    # æ„å»ºç¬¦åˆå¾®è°ƒæ ¼å¼çš„è¾“å…¥
    full_prompt = system_prompt + f"user\n{prompt}\nassistant\n"
    
    # ç”¨æˆ·æ¶ˆæ¯å±•ç¤º
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)

    # ç”Ÿæˆå›ç­”
    with st.chat_message("assistant"):
        with st.spinner("æ­£åœ¨ç”Ÿæˆå›ç­”..."):
            # ç¼–ç è¾“å…¥
            inputs = tokenizer(full_prompt, return_tensors="pt").to(model.device)
            
            # ç”Ÿæˆå‚æ•°é…ç½®
            generate_kwargs = {
                "inputs": inputs.input_ids,
                "max_new_tokens": max_new_tokens,
                "temperature": temperature,
                "top_p": top_p,
                "do_sample": True,
                "pad_token_id": tokenizer.eos_token_id,
                "repetition_penalty": 1.1  # æ·»åŠ é‡å¤æƒ©ç½š
            }
            
            # ç”Ÿæˆå“åº”
            outputs = model.generate(**generate_kwargs)
            response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # æ‰“å°å®Œæ•´çš„responseï¼ˆç”¨äºè°ƒè¯•ï¼‰
            print("å®Œæ•´çš„responseï¼š", response)
            
            # æå–ç”Ÿæˆçš„å›ç­”éƒ¨åˆ†ï¼ˆç¡®ä¿åªæå–assistantéƒ¨åˆ†ï¼‰
            if "assistant\n" in response:
                # æå–assistantéƒ¨åˆ†
                answer = response.split("assistant\n")[-1].strip()
            else:
                # å¦‚æœæ ¼å¼ä¸ç¬¦åˆé¢„æœŸï¼Œç›´æ¥ä½¿ç”¨å®Œæ•´è¾“å‡º
                answer = response.strip()

        # å±•ç¤ºå›ç­”
        st.write(answer)
        st.session_state.messages.append({"role": "assistant", "content": answer})


# ä»˜è´¹åŠŸèƒ½çš„è¯´æ˜å’ŒæŒ‰é’®
st.markdown("---")
st.markdown("**ä»˜è´¹åŠŸèƒ½**")
st.markdown("ä»¥ä¸‹æ˜¯æˆ‘ä»¬çš„ä»˜è´¹åŠŸèƒ½ï¼š")
st.markdown("- **æ™ºèƒ½æŠ•é¡¾åŠ©æ‰‹**ï¼šä¸ºæ‚¨æä¾›ä¸“ä¸šçš„æŠ•èµ„å»ºè®®å’Œèµ„äº§é…ç½®æ–¹æ¡ˆã€‚ï¼ˆé¦–æœˆä»…éœ€9.9å…ƒï¼‰")
st.markdown("- **AIåˆ¶ä½œPPT**ï¼šæ ¹æ®æ‚¨çš„éœ€æ±‚è‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡çš„PPTã€‚ï¼ˆä¸€æ¬¡åˆ¶ä½œä»…éœ€5å…ƒï¼‰")
st.markdown("- **è®ºæ–‡æŸ¥é‡**ï¼šä¸ºæ‚¨æä¾›å¿«é€Ÿå‡†ç¡®çš„è®ºæ–‡æŸ¥é‡æœåŠ¡ã€‚ï¼ˆä¸€ä¸‡å­—10å…ƒï¼Œæˆ‘ä»¬æ‹¥æœ‰è¿œè¶…å…¶ä»–å¹³å°çš„å“è´¨å’Œæé«˜çš„æ€§ä»·æ¯”ï¼‰")

st.markdown("---")
st.markdown("**ç«‹å³ä»˜è´¹**")
st.markdown("[å‰å¾€ä»˜è´¹é¡µé¢](https://www.cufe-aiteam.com/pay)")
st.markdown("å¦‚æœæ‚¨å·²ç»æ˜¯ä»˜è´¹ç”¨æˆ·ï¼Œè¯·è¾“å…¥æ‚¨å¯¹åº”ä»˜è´¹åŠŸèƒ½çš„å‡­è¯ï¼š")
paid_code = st.text_input("ä»˜è´¹å‡­è¯")
if st.button("éªŒè¯"):
    if paid_code == "your_paid_code":  # æ›¿æ¢ä¸ºå®é™…çš„ä»˜è´¹å‡­è¯éªŒè¯é€»è¾‘
        st.success("éªŒè¯æˆåŠŸï¼æ‚¨å·²æˆåŠŸè§£é”ä»˜è´¹åŠŸèƒ½ã€‚")
    else:
        st.error("éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ‚¨çš„ä»˜è´¹å‡­è¯ã€‚")
