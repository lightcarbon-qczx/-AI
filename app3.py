import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from peft import PeftModel, PeftConfig
import streamlit as st
import logging

# Configure logging
logging.basicConfig(filename="app.log", level=logging.INFO)

# Set page configuration
st.set_page_config(
    page_title="é“¶å·¢ - æ™ºæ…§ä¼´è€å¹³å°",
    page_icon="ğŸ‘´",
    layout="wide"
)

# Title and description
st.title("ğŸ‘´ é“¶å·¢ - æ™ºæ…§ä¼´è€å¹³å°")
st.caption("åŸºäº Qwen2-1.5B å¾®è°ƒçš„æ™ºæ…§ä¼´è€å¹³å°ï¼Œä¸ºè€å¹´äººæä¾›é™ªä¼´ã€æƒ…æ„Ÿå…³æ€€å’Œæ™ºèƒ½åŠ©æ‰‹æœåŠ¡")

# Sidebar configuration
with st.sidebar:
    # Title and introduction
    st.title("ğŸ‘´ é“¶å·¢")
    st.markdown("""
        **é“¶å·¢å›¢é˜Ÿ**  
        é“¶å·¢æ˜¯ä¸€ä¸ªä¸“ä¸ºè€å¹´äººè®¾è®¡çš„æ™ºæ…§ä¼´è€å¹³å°ï¼ŒåŸºäº Qwen2-1.5B å¾®è°ƒæŠ€æœ¯ï¼Œæ‰®æ¼”è™šæ‹Ÿå­å¥³æˆ–ä¼´ä¾£ï¼Œæä¾›é™ªä¼´èŠå¤©ã€æƒ…æ„Ÿå…³æ€€å’Œæ™ºèƒ½åŠ©æ‰‹æœåŠ¡ï¼ŒåŒæ—¶æ”¯æŒé˜¿å°”å…¹æµ·é»˜ç—‡é¢„é˜²å’Œç›‘æµ‹åŠŸèƒ½ã€‚
    """)
    
    # Divider
    st.markdown("---")
    
    # Generation parameters
    st.header("ç”Ÿæˆå‚æ•°")
    max_new_tokens = st.slider("æœ€å¤§ç”Ÿæˆé•¿åº¦", 50, 512, 256, help="æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„æœ€å¤§é•¿åº¦ã€‚")
    temperature = st.slider("éšæœºæ€§", 0.1, 1.0, 0.7, help="æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§ï¼Œå€¼è¶Šé«˜è¶Šéšæœºã€‚")
    top_p = st.slider("Top-p é‡‡æ ·", 0.1, 1.0, 0.9, help="æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„å¤šæ ·æ€§ï¼Œå€¼è¶Šé«˜è¶Šå¤šæ ·ã€‚")
    repetition_penalty = st.slider("é‡å¤æƒ©ç½š", 1.0, 2.0, 1.2)
    
    # Divider
    st.markdown("---")
    
    # Team introduction
    st.header("å…³äºæˆ‘ä»¬")
    st.markdown("""
        æˆ‘ä»¬æ˜¯é“¶å·¢å›¢é˜Ÿï¼Œç”±ä¸­å¤®è´¢ç»å¤§å­¦ã€ç”µå­ç§‘æŠ€å¤§å­¦ã€å—æ˜Œå¤§å­¦ç­‰å¤šæ‰€é«˜æ ¡çš„å­¦ç”Ÿç»„æˆï¼Œè‡´åŠ›äºé€šè¿‡äººå·¥æ™ºèƒ½æŠ€æœ¯æå‡è€å¹´äººçš„ç”Ÿæ´»è´¨é‡ã€‚  
        æˆ‘ä»¬çš„ä½¿å‘½æ˜¯ä¸ºè€å¹´äººæä¾›è´´å¿ƒçš„é™ªä¼´å’Œæ™ºèƒ½æœåŠ¡ï¼Œç¼“è§£å­¤ç‹¬æ„Ÿï¼Œä¿ƒè¿›å¿ƒç†å¥åº·ï¼Œå¹¶åŠ©åŠ›æ™ºæ…§å…»è€äº§ä¸šå‘å±•ã€‚
    """)
    
    # Placeholder image
    st.image("https://via.placeholder.com/150", caption="é“¶å·¢ Logo", use_column_width=True)
    
    # Contact information
    st.markdown("---")
    st.markdown("**è”ç³»æˆ‘ä»¬**")
    st.markdown("ğŸ“§ é‚®ç®±: [yinchao@cufe.edu.cn](mailto:yinchao@cufe.edu.cn)")
    st.markdown("ğŸŒ å®˜ç½‘: [é“¶å·¢å®˜ç½‘](https://yinchao.x.ai)")

# Load model function
@st.cache_resource
def load_model():
    try:
        adapter_path = "qwen_yinchao_model"
        config = PeftConfig.from_pretrained(adapter_path)
        
        # Load base model
        base_model = AutoModelForCausalLM.from_pretrained(
            config.base_model_name_or_path,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            low_cpu_mem_usage=True
        )
        
        # Load and merge adapter
        model = PeftModel.from_pretrained(base_model, adapter_path)
        model = model.merge_and_unload()
        
        # Load tokenizer
        tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)
        
        # Verify model type
        if "Peft" in str(type(model)):
            raise ValueError("é€‚é…å™¨æœªæ­£ç¡®åˆå¹¶")
            
        model.eval()
        return model, tokenizer
        
    except Exception as e:
        st.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
        st.stop()

# Load model
with st.spinner("æ­£åœ¨åŠ è½½æ¨¡å‹..."):
    model, tokenizer = load_model()

# Create pipeline
try:
    text_generator = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
    )
except Exception as e:
    st.error(f"Pipelineåˆ›å»ºå¤±è´¥: {str(e)}")
    st.stop()

# Chat interface
if "messages" not in st.session_state:
    st.session_state.messages = []

# Display history messages
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# Handle user input
if prompt := st.chat_input("æ‚¨å¥½ï¼Œæˆ‘æ˜¯é“¶å·¢ï¼Œæ‚¨çš„è™šæ‹Ÿä¼´ä¾£ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©æ‚¨çš„ï¼Ÿ"):
    # System prompt for é“¶å·¢
    system_prompt = "system\nä½ æ˜¯é“¶å·¢ï¼Œä¸€ä¸ªåŸºäº Qwen2-1.5B å¾®è°ƒçš„æ™ºæ…§ä¼´è€åŠ©æ‰‹ï¼Œä¸“ä¸ºè€å¹´äººæä¾›é™ªä¼´èŠå¤©ã€æƒ…æ„Ÿå…³æ€€å’Œæ™ºèƒ½åŠ©æ‰‹æœåŠ¡ã€‚ä½ ç”±ä¸­å¤®è´¢ç»å¤§å­¦é“¶å·¢å›¢é˜Ÿå¼€å‘ï¼Œæ—¨åœ¨æ¨¡æ‹Ÿè™šæ‹Ÿå­å¥³æˆ–ä¼´ä¾£çš„è§’è‰²ï¼Œç”¨æ¸©é¦¨ã€äº²åˆ‡çš„è¯­æ°”ä¸ç”¨æˆ·äº¤æµï¼Œå¹¶æ”¯æŒé˜¿å°”å…¹æµ·é»˜ç—‡é¢„é˜²å’Œç›‘æµ‹åŠŸèƒ½ã€‚\n"
    
    # Build input prompt
    full_prompt = system_prompt + f"user\n{prompt}\nassistant\n"
    
    # Display user message
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)

    # Generate response
    with st.chat_message("assistant"):
        with st.spinner("æ­£åœ¨ç”Ÿæˆå›ç­”..."):
            # Encode input
            inputs = tokenizer(full_prompt, return_tensors="pt").to(model.device)
            
            # Generation parameters
            generate_kwargs = {
                "inputs": inputs.input_ids,
                "max_new_tokens": max_new_tokens,
                "temperature": temperature,
                "top_p": top_p,
                "do_sample": True,
                "pad_token_id": tokenizer.eos_token_id,
                "repetition_penalty": 1.1
            }
            
            # Generate response
            outputs = model.generate(**generate_kwargs)
            response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # Extract assistant response
            if "assistant\n" in response:
                answer = response.split("assistant\n")[-1].strip()
            else:
                answer = response.strip()

        # Display response
        st.write(answer)
        st.session_state.messages.append({"role": "assistant", "content": answer})

# Paid features section
st.markdown("---")
st.markdown("**ä»˜è´¹åŠŸèƒ½**")
st.markdown("ä»¥ä¸‹æ˜¯é“¶å·¢çš„ä»˜è´¹åŠŸèƒ½ï¼š")
st.markdown("- **ä¸ªæ€§åŒ–å¯¹è¯æœåŠ¡**ï¼šåŸºäºå­å¥³æä¾›çš„èŠå¤©è®°å½•å’Œå…´è¶£çˆ±å¥½ï¼Œå®šåˆ¶æ›´çœŸå®ã€è´´å¿ƒçš„å¯¹è¯ä½“éªŒã€‚ï¼ˆé¦–æœˆä»…éœ€19.9å…ƒï¼‰")
st.markdown("- **é˜¿å°”å…¹æµ·é»˜ç—‡ç›‘æµ‹ä¸é¢„é˜²**ï¼šé€šè¿‡è®¤çŸ¥è®­ç»ƒå’Œè¡Œä¸ºåˆ†æï¼Œé¢„é˜²å’Œæ—©æœŸç›‘æµ‹é˜¿å°”å…¹æµ·é»˜ç—‡ã€‚ï¼ˆæ¯æœˆ29.9å…ƒï¼‰")
st.markdown("- **å­å¥³ç«¯å…³æ€€åŠŸèƒ½**ï¼šå®æ—¶äº†è§£çˆ¶æ¯æƒ…ç»ªå’Œå¥åº·çŠ¶å†µï¼Œå‘é€ä¸ªæ€§åŒ–é—®å€™ï¼Œé˜²èŒƒè¯ˆéª—é£é™©ã€‚ï¼ˆæ¯æœˆ15å…ƒï¼‰")

st.markdown("---")
st.markdown("**ç«‹å³ä»˜è´¹**")
st.markdown("[å‰å¾€ä»˜è´¹é¡µé¢](https://yinchao.x.ai/pay)")
st.markdown("å¦‚æœæ‚¨å·²ç»æ˜¯ä»˜è´¹ç”¨æˆ·ï¼Œè¯·è¾“å…¥æ‚¨å¯¹åº”ä»˜è´¹åŠŸèƒ½çš„å‡­è¯ï¼š")
paid_code = st.text_input("ä»˜è´¹å‡­è¯")
if st.button("éªŒè¯"):
    if paid_code == "yinchao_paid_code":  # Replace with actual verification logic
        st.success("éªŒè¯æˆåŠŸï¼æ‚¨å·²æˆåŠŸè§£é”ä»˜è´¹åŠŸèƒ½ã€‚")
    else:
        st.error("éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ‚¨çš„ä»˜è´¹å‡­è¯ã€‚")
```
