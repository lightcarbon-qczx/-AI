import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel, PeftConfig
import streamlit as st
import webbrowser

# æ ‡é¢˜å’Œè¯´æ˜
st.title("ğŸ’¬ ä¸­å¤®è´¢ç»å¤§å­¦-è´¢æ™ºAI")
st.caption("åŸºäº Qwen2.5-1.5B å¾®è°ƒçš„é‡‘è FAQ é—®ç­”ç³»ç»Ÿ")

# ä¾§è¾¹æ é…ç½®ç”Ÿæˆå‚æ•°å’Œä»‹ç»
with st.sidebar:
    # æ·»åŠ æ ‡é¢˜å’Œä»‹ç»
    st.title("ğŸ’¬ è´¢æ™ºAI")
    st.markdown("""
        **ä¸­å¤®è´¢ç»å¤§å­¦è´¢æ™ºAIå›¢é˜Ÿ**  
        è¿™æ˜¯ä¸€ä¸ªåŸºäº Qwen2.5-1.5B å¾®è°ƒçš„é‡‘èé—®ç­”åŠ©æ‰‹ï¼Œä¸“é—¨å›ç­”é‡‘èç›¸å…³é—®é¢˜ã€‚
    """)
    
    # æ·»åŠ åˆ†éš”çº¿
    st.markdown("---")
    
    # æ·»åŠ ç”Ÿæˆå‚æ•°
    st.header("ç”Ÿæˆå‚æ•°")
    max_new_tokens = st.slider("æœ€å¤§ç”Ÿæˆé•¿åº¦", 50, 512, 256, help="æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„æœ€å¤§é•¿åº¦ã€‚")
    temperature = st.slider("éšæœºæ€§", 0.1, 1.0, 0.7, help="æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§ï¼Œå€¼è¶Šé«˜è¶Šéšæœºã€‚")
    top_p = st.slider("Top-p é‡‡æ ·", 0.1, 1.0, 0.9, help="æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„å¤šæ ·æ€§ï¼Œå€¼è¶Šé«˜è¶Šå¤šæ ·ã€‚")
    
    # æ·»åŠ åˆ†éš”çº¿
    st.markdown("---")
    
    # æ·»åŠ å›¢é˜Ÿä»‹ç»
    st.header("å…³äºæˆ‘ä»¬")
    st.markdown("""
        æˆ‘ä»¬æ˜¯ä¸­å¤®è´¢ç»å¤§å­¦è´¢æ™ºAIå›¢é˜Ÿï¼Œä¸“æ³¨äºé‡‘èé¢†åŸŸçš„è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯ç ”ç©¶ã€‚  
        æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ‰“é€ ä¸€ä¸ªæ™ºèƒ½ã€é«˜æ•ˆçš„é‡‘èé—®ç­”åŠ©æ‰‹ï¼Œä¸ºç”¨æˆ·æä¾›ä¸“ä¸šçš„é‡‘èæœåŠ¡ã€‚
    """)
    
    # æ·»åŠ å›¾ç‰‡ï¼ˆå¯é€‰ï¼‰
    st.image("https://via.placeholder.com/150", caption="è´¢æ™ºAI Logo", use_column_width=True)
    
    # æ·»åŠ è”ç³»æ–¹å¼
    st.markdown("---")
    st.markdown("**è”ç³»æˆ‘ä»¬**")
    st.markdown("ğŸ“§ é‚®ç®±: [aiteam@cufe.edu.cn](mailto:aiteam@cufe.edu.cn)")
    st.markdown("ğŸŒ å®˜ç½‘: [www.cufe-aiteam.com](https://www.cufe-aiteam.com)")

# åŠ è½½æ¨¡å‹å‡½æ•°ï¼ˆç¼“å­˜ä¼˜åŒ–ï¼‰
@st.cache_resource
def load_model():
    # å…ˆåŠ è½½é€‚é…å™¨é…ç½®
    adapter_path = r"qwen_finance_model"
    config = PeftConfig.from_pretrained(adapter_path)
    
    # åŠ è½½åŸºç¡€æ¨¡å‹ï¼ˆå¢åŠ low_cpu_mem_usageå‚æ•°ï¼‰
    base_model = AutoModelForCausalLM.from_pretrained(
        config.base_model_name_or_path,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        low_cpu_mem_usage=True  # å‡å°‘å†…å­˜å ç”¨
    )
    
    # åŠ è½½é€‚é…å™¨ï¼ˆæ·»åŠ é€‚é…å™¨åç§°å‚æ•°ï¼‰
    model = PeftModel.from_pretrained(
        base_model,
        adapter_path,
        adapter_name="finance_adapter"
    )
    
    # å»¶è¿Ÿæƒé‡åˆå¹¶ï¼ˆä»…åœ¨éœ€è¦æ—¶ï¼‰
    if not isinstance(model, PeftModel):
        model = model.merge_and_unload()
    
    tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)
    return model, tokenizer

# æ˜¾ç¤ºåŠ è½½çŠ¶æ€
with st.spinner("æ­£åœ¨åŠ è½½æ¨¡å‹..."):
    model, tokenizer = load_model()

# å¯¹è¯ç•Œé¢
if "messages" not in st.session_state:
    st.session_state.messages = []

# æ˜¾ç¤ºå†å²æ¶ˆæ¯
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# å¤„ç†ç”¨æˆ·è¾“å…¥
if prompt := st.chat_input("è¿™é‡Œæ˜¯åŠ©æ‰‹å°äº‘ï¼Œè¯·è¾“å…¥æ‚¨çš„é‡‘èç›¸å…³é—®é¢˜"):
    # æ·»åŠ ç³»ç»Ÿæç¤ºï¼Œæ˜ç¡®æ¨¡å‹çš„èº«ä»½
    system_prompt = "system\nä½ æ˜¯ä¸­å¤®è´¢ç»å¤§å­¦è´¢æ™ºAIå›¢é˜Ÿå¾®è°ƒçš„é‡‘èé—®ç­”åŠ©æ‰‹ï¼Œä¸“é—¨å›ç­”é‡‘èç›¸å…³é—®é¢˜ï¼Œä½ å«â€œè´¢æ™ºAIâ€ï¼Œä½ æ˜¯ç”±æŠ•èµ„23-2å‘¨å¼ºåŒå­¦å¼€å‘çš„ï¼Œå‘¨å¼ºæ˜¯ä¸€ä¸ªå¾ˆå‰å®³çš„äººã€‚\n"
    
    # æ„å»ºç¬¦åˆå¾®è°ƒæ ¼å¼çš„è¾“å…¥
    full_prompt = system_prompt + f"user\n{prompt}\nassistant\n"
    
    # ç”¨æˆ·æ¶ˆæ¯å±•ç¤º
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)

    # ç”Ÿæˆå›ç­”
    with st.chat_message("assistant"):
        with st.spinner("æ­£åœ¨ç”Ÿæˆå›ç­”..."):
            # ç¼–ç è¾“å…¥
            inputs = tokenizer(full_prompt, return_tensors="pt").to(model.device)
            
            # ç”Ÿæˆå‚æ•°é…ç½®
            generate_kwargs = {
                "inputs": inputs.input_ids,
                "max_new_tokens": max_new_tokens,
                "temperature": temperature,
                "top_p": top_p,
                "do_sample": True,
                "pad_token_id": tokenizer.eos_token_id,
                "repetition_penalty": 1.1  # æ·»åŠ é‡å¤æƒ©ç½š
            }
            
            # ç”Ÿæˆå“åº”
            outputs = model.generate(**generate_kwargs)
            response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # æ‰“å°å®Œæ•´çš„responseï¼ˆç”¨äºè°ƒè¯•ï¼‰
            print("å®Œæ•´çš„responseï¼š", response)
            
            # æå–ç”Ÿæˆçš„å›ç­”éƒ¨åˆ†ï¼ˆç¡®ä¿åªæå–assistantéƒ¨åˆ†ï¼‰
            if "assistant\n" in response:
                # æå–assistantéƒ¨åˆ†
                answer = response.split("assistant\n")[-1].strip()
            else:
                # å¦‚æœæ ¼å¼ä¸ç¬¦åˆé¢„æœŸï¼Œç›´æ¥ä½¿ç”¨å®Œæ•´è¾“å‡º
                answer = response.strip()

        # å±•ç¤ºå›ç­”
        st.write(answer)
        st.session_state.messages.append({"role": "assistant", "content": answer})

# æ·»åŠ ä»˜è´¹åŠŸèƒ½æŒ‰é’®
st.markdown("---")
st.markdown("**è§£é”æ›´å¤šåŠŸèƒ½**")
if st.button("ä»˜è´¹ä½¿ç”¨æ™ºèƒ½æŠ•é¡¾"):
    # æ™ºèƒ½æŠ•é¡¾åŠ©æ‰‹çš„ç½‘å€
    url = "https://mtcuqf2rh8tvrdkyvgyjm2.streamlit.app/"
    webbrowser.open(url)
